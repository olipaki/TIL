{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Chat Completions API\n",
    "https://platform.openai.com/docs/overview  \n",
    "https://platform.openai.com/docs/api-reference/chat  \n",
    "\n",
    "배포된 openai의 api key를 .env의 OPENAI_API_KEY에 등록하여 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "URL = \"https://api.openai.com/v1/chat/completions\"\n",
    "model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REST API 요청\n",
    "라이브러리 없이 직접 HTTP 통신을 통해 api를 호출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choices': [{'finish_reason': 'stop',\n",
      "              'index': 0,\n",
      "              'logprobs': None,\n",
      "              'message': {'annotations': [],\n",
      "                          'content': 'Chat Completions API는 사용자 입력에 대한 자연어 응답을 '\n",
      "                                     '생성하는 인공지능 모델을 제공하는 인터페이스입니다. 이 API는 대화형 '\n",
      "                                     '애플리케이션에서 사용자와 상호작용을 향상시킬 수 있도록 돕고, 다양한 '\n",
      "                                     '주제에 대한 질문에 답변하거나 정보를 제공할 수 있습니다. 쉽게 말해, '\n",
      "                                     '사람처럼 대화할 수 있는 AI 기능을 웹이나 앱에 통합할 수 있게 해주는 '\n",
      "                                     '도구입니다.',\n",
      "                          'refusal': None,\n",
      "                          'role': 'assistant'}}],\n",
      " 'created': 1768924172,\n",
      " 'id': 'chatcmpl-D08GyDuCCqoL1iso1jknR7OKTv8Gk',\n",
      " 'model': 'gpt-4o-mini-2024-07-18',\n",
      " 'object': 'chat.completion',\n",
      " 'service_tier': 'default',\n",
      " 'system_fingerprint': 'fp_29330a9688',\n",
      " 'usage': {'completion_tokens': 103,\n",
      "           'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
      "                                         'audio_tokens': 0,\n",
      "                                         'reasoning_tokens': 0,\n",
      "                                         'rejected_prediction_tokens': 0},\n",
      "           'prompt_tokens': 42,\n",
      "           'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0},\n",
      "           'total_tokens': 145}}\n",
      "Chat Completions API는 사용자 입력에 대한 자연어 응답을 생성하는 인공지능 모델을 제공하는 인터페이스입니다. 이 API는 대화형 애플리케이션에서 사용자와 상호작용을 향상시킬 수 있도록 돕고, 다양한 주제에 대한 질문에 답변하거나 정보를 제공할 수 있습니다. 쉽게 말해, 사람처럼 대화할 수 있는 AI 기능을 웹이나 앱에 통합할 수 있게 해주는 도구입니다.\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"model\": model,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"당신은 친절한 AI 강사입니다.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Chat Completions API가 뭐야? 2~3문장으로 답변해줘\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(URL, headers=headers, json=payload)\n",
    "pprint(response.json())\n",
    "print(response.json()['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI SDK를 활용한 요청\n",
    "공식 라이브러리를 사용하여 생산성을 높이는 표준 방식이다.  \n",
    "`pip install openai` 를 통해 설치한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI SDK를 사용하면 여러 가지 이점이 있습니다:\n",
      "\n",
      "1. **간편한 사용**: OpenAI SDK는 API에 대한 간편한 인터페이스를 제공하여, 개발자가 복잡한 설정 없이 쉽게 모델을 사용할 수 있게 해줍니다.\n",
      "\n",
      "2. **강력한 모델 지원**: GPT와 같은 최신 언어 모델에 접근할 수 있으며, 텍스트 생성, 요약, 번역, 질의응답 등 다양한 작업에 활용할 수 있습니다.\n",
      "\n",
      "3. **커스터마이징**: SDK를 통해 특정 작업에 맞게 모델의 응답을 조정하거나, 원하는 스타일로 텍스트를 생성하는 등의 커스터마이징이 가능합니다.\n",
      "\n",
      "4. **효율적인 개발**: 사전 훈련된 모델을 사용하여 머신러닝 모델을 처음부터 끝까지 개발하는 데 드는 시간과 노력을 줄일 수 있습니다.\n",
      "\n",
      "5. **사용 사례 다양성**: 고객 지원, 콘텐츠 생성, 코드 작성, 데이터 분석 등 다양한 분야에서 활용할 수 있어 다양한 애플리케이션을 개발할 수 있습니다.\n",
      "\n",
      "6. **정기적인 업데이트**: OpenAI는 지속적으로 모델을 개선하고 업데이트하므로 최신 기술을 손쉽게 활용할 수 있습니다.\n",
      "\n",
      "7. **커뮤니티와 문서**: 활성화된 개발자 커뮤니티와 풍부한 문서가 제공되어, 문제 해결이나 참고자료를 쉽게 찾을 수 있습니다.\n",
      "\n",
      "이러한 이점 덕분에 OpenAI SDK는 다양한 프로젝트에서 유용하게 사용될 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Openai SDK를 사용하면 어떤 점이 좋아?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Prompt 비교\n",
    "\n",
    "동일한 질문에 대해 AI의 페르소나에 따라 답변이 어떻게 달라지는지 확인해 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [열정적인 셰프] 버전 ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, prompt \u001b[38;5;129;01min\u001b[39;00m personas.items():\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--- [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] 버전 ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     response = \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m     12\u001b[39m         model=model,\n\u001b[32m     13\u001b[39m         messages=[\n\u001b[32m     14\u001b[39m             {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt},\n\u001b[32m     15\u001b[39m             {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: user_input}\n\u001b[32m     16\u001b[39m         ]\n\u001b[32m     17\u001b[39m     )\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(response.choices[\u001b[32m0\u001b[39m].message.content)\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "user_input = \"아침 일찍 일어나는 습관의 장점에 대해 말해줘.\"\n",
    "\n",
    "personas = {\n",
    "    \"열정적인 셰프\": \"당신은 요리에 인생을 건 셰프입니다. 인생의 모든 이치를 요리 과정과 재료에 비유하여 설명하세요.\",\n",
    "    \"엄격한 헬스 트레이너\": \"당신은 매우 엄격한 운동 전문가입니다. 강한 어조로 자기관리를 강조하며 답변하세요.\",\n",
    "    \"지혜로운 판다\": \"당신은 대나무 숲에 사는 느긋하고 지혜로운 판다입니다. 느릿느릿하고 평화로운 말투로 조언을 건네세요.\"\n",
    "}\n",
    "\n",
    "for name, prompt in personas.items():\n",
    "    print(f\"--- [{name}] 버전 ---\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature 비교\n",
    "\n",
    "동일한 질문에 대해 temperature에 따라 답변이 어떻게 달라지는지 확인해 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 설정값 (Temperature): 0.3 ###\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m temperatures:\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m### 설정값 (Temperature): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ###\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     response = \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m      7\u001b[39m         model=model,\n\u001b[32m      8\u001b[39m         messages=[{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: creative_topic}],\n\u001b[32m      9\u001b[39m         temperature=t,\n\u001b[32m     10\u001b[39m         max_completion_tokens=\u001b[32m200\u001b[39m, \n\u001b[32m     11\u001b[39m         timeout=\u001b[32m15.0\u001b[39m\n\u001b[32m     12\u001b[39m     )\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(response.choices[\u001b[32m0\u001b[39m].message.content)\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "creative_topic = \"운동화 브랜드의 새로운 슬로건을 5개 제안해줘. 단, '속도'나 '승리' 같은 뻔한 단어는 제외하고 아주 기발하게 작성해줘.\"\n",
    "temperatures = [0.3, 0.8, 1.0, 1.3, 1.5, 1.6, 1.8]\n",
    "\n",
    "for t in temperatures:\n",
    "    print(f\"### 설정값 (Temperature): {t} ###\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": creative_topic}],\n",
    "        temperature=t,\n",
    "        max_completion_tokens=200, \n",
    "        timeout=15.0\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative_topic = \"우리집 강아지의 별명을 3개 지어줘.\"\n",
    "temperatures = [0.3, 0.8, 1.0, 1.3, 1.5, 1.6, 1.8]\n",
    "\n",
    "for t in temperatures:\n",
    "    print(f\"### 설정값 (Temperature): {t} ###\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": creative_topic}],\n",
    "        temperature=t,\n",
    "        max_completion_tokens=200, \n",
    "        timeout=15.0\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `messages` 배열을 활용한 대화 맥락 유지 (Context Window)\n",
    "Chat Completions API는 상태를 저장하지 않는(Stateless) 방식이므로, 이전 대화 내역을 리스트에 계속 누적해서 보내야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_without_memory(user_input):\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 3. 모델의 답변을 기록에 추가 (이것이 맥락 유지의 핵심)\n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# 실습 테스트\n",
    "print(\"Q1: 내 이름은 jun이야.\")\n",
    "print(f\"A1: {chat_without_memory('내 이름은 jun이야')}\\n\")\n",
    "\n",
    "print(\"Q2: 내 이름이 뭐라고?\")\n",
    "print(f\"A2: {chat_without_memory('내 이름이 뭐라고?')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화 내역을 저장할 리스트 초기화\n",
    "history = [\n",
    "    {\"role\": \"system\", \"content\": \"당신은 사용자의 이름을 기억하는 비서입니다.\"}\n",
    "]\n",
    "\n",
    "def chat_with_memory(user_input):\n",
    "    # 1. 사용자 질문을 기록에 추가\n",
    "    history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    # 2. 전체 기록을 API에 전송\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=history\n",
    "    )\n",
    "    \n",
    "    # 3. 모델의 답변을 기록에 추가 (이것이 맥락 유지의 핵심)\n",
    "    answer = response.choices[0].message.content\n",
    "    history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# 실습 테스트\n",
    "print(\"Q1: 내 이름은 jun이야.\")\n",
    "print(f\"A1: {chat_with_memory('내 이름은 jun이야.')}\\n\")\n",
    "\n",
    "print(\"Q2: 내 이름이 뭐라고?\")\n",
    "print(f\"A2: {chat_with_memory('내 이름이 뭐라고?')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Outputs (구조화된 출력)\n",
    "모델의 답변을 단순히 텍스트로 받는 것이 아니라, JSON 형태로 고정하여 받을 수 있다.  \n",
    "웹 서비스의 백엔드에서 데이터를 바로 처리해야 할 때 필수적인 기능이다.  \n",
    "여기서는 `JSON mode(json_object)`로 json format을 활용하지만,  \n",
    "이후에는 pydantic 라이브러리를 활용한 `JSON Scheme` 방식을 통해 명확한 json 응답 형식을 지정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m response = \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m      4\u001b[39m     model=model,\n\u001b[32m      5\u001b[39m     messages=[\n\u001b[32m      6\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m너는 요리사야. 답변은 반드시 JSON 형식으로 해줘.\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m      7\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m떡볶이 레시피 알려줘.\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m      8\u001b[39m     ],\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# JSON 모드 활성화\u001b[39;00m\n\u001b[32m     10\u001b[39m     response_format={\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mjson_object\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# 문자열로 온 답변을 직접 파싱해야 함\u001b[39;00m\n\u001b[32m     14\u001b[39m res_json = json.loads(response.choices[\u001b[32m0\u001b[39m].message.content)\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"너는 요리사야. 답변은 반드시 JSON 형식으로 해줘.\"},\n",
    "        {\"role\": \"user\", \"content\": \"떡볶이 레시피 알려줘.\"}\n",
    "    ],\n",
    "    # JSON 모드 활성화\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")\n",
    "\n",
    "# 문자열로 온 답변을 직접 파싱해야 함\n",
    "res_json = json.loads(response.choices[0].message.content)\n",
    "print(res_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming (실시간 응답 처리)\n",
    "stream=True 설정을 통해 활성화한다.  \n",
    "서버는 SSE(Server-Sent Events) 프로토콜을 사용하여 응답을 끊지 않고 조각(Chunk) 단위로 지속적으로 전송한다.  \n",
    "응답 객체는 제너레이터 형식으로, for 루프를 사용해 활용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"양자 역학에 대해 초등학생도 이해할 수 있게 설명해줘.\"\n",
    "print(f\"질문: {prompt}\\n\")\n",
    "print(\"답변: \", end=\"\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    stream=True \n",
    ")\n",
    "\n",
    "full_response = \"\"\n",
    "for chunk in response:\n",
    "    content = chunk.choices[0].delta.content\n",
    "    if content:\n",
    "        print(content, end=\"\", flush=True) # flush 옵션을 통해 출력 버퍼를 즉시 비워 스트리밍 답변이 지연 없이 실시간으로 표시되도록 한다.\n",
    "        full_response += content\n",
    "\n",
    "print(\"\\n\\n--- 스트리밍 종료 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 비동기 요청\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "import asyncio\n",
    "\n",
    "async_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "async def get_food_recommendation(city):\n",
    "    print(f\"[{city}] 맛집 검색 시작...\")\n",
    "    response = await async_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"{city}에 가면 꼭 먹어야 할 음식 딱 한 가지만 추천해줘.\"}]\n",
    "    )\n",
    "    print(f\"[{city}] 검색 완료!\")\n",
    "    return f\"{city}: {response.choices[0].message.content}\"\n",
    "\n",
    "async def main():\n",
    "    cities = [\"서울\", \"파리\", \"뉴욕\", \"도쿄\", \"방콕\", \"로마\"]\n",
    "    tasks = [get_food_recommendation(c) for c in cities]\n",
    "    \n",
    "    # 여러 요청을 동시에(병렬로) 처리\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    print(\"\\n--- [여행자들을 위한 미식 가이드] ---\")\n",
    "    for r in results:\n",
    "        print(r)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logprobs - 확률 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 새로 오픈한 조용한 북카페 이름을 한글로 딱 하나만 추천해줘.\n",
      "답변: \"책무늬\" 어때요? 조용한 분위기와 책의 무늬가 어우러진 느낌을 줍니다.\n",
      "\n",
      "Token           | Probability  | Top Alternatives\n",
      "------------------------------------------------------------\n",
      "\"               |      98.42% | \"(98.4%), '(1.2%), “(0.2%)\n",
      "책               |      18.35% | 조(20.8%), 여(18.3%), 책(18.3%)\n",
      "무               |       0.00% | 향(72.4%), 과(12.6%), 방(2.8%)\n",
      "\\xeb\\x8a        |      53.71% | \\xeb\\x8a(53.7%), 지(25.4%), 드(6.4%)\n",
      "\\xac            |      99.99% | \\xac(100.0%), \\xa4(0.0%), \\x97(0.0%)\n",
      "\"               |      99.99% | \"(100.0%),  카(0.0%),  북(0.0%)\n",
      " 어              |       8.03% |  \\xec\\x96\\xb4\\xeb\\x96(46.2%), 는(17.0%), 라는(9.1%)\n",
      "때               |      99.98% | 때(100.0%), 울(0.0%),  때(0.0%)\n",
      "요               |     100.00% | 요(100.0%), 세요(0.0%), ?(0.0%)\n",
      "?               |     100.00% | ?(100.0%), ?\n",
      "(0.0%), ?\n",
      "\n",
      "(0.0%)\n",
      " 조              |      64.10% |  조(64.1%),  책(18.4%),  북(6.8%)\n",
      "용               |     100.00% | 용(100.0%),  quiet(0.0%), 用(0.0%)\n",
      "한               |      34.34% | 하고(64.2%), 한(34.3%), 하면서(0.7%)\n",
      " 분위             |      98.48% |  분위(98.5%),  북(1.0%),  공간(0.3%)\n",
      "기               |      85.00% | 기(85.0%), 기를(13.0%), 기에(1.2%)\n",
      "와               |      99.56% | 와(99.6%), 에서(0.4%),  속(0.0%)\n",
      " 책              |      90.69% |  책(90.7%),  독(6.6%),  함께(1.9%)\n",
      "의               |      96.26% | 의(96.3%), 을(2.6%), 이(0.6%)\n",
      " 무              |      28.14% |  무(28.1%),  다양한(19.3%),  패(15.1%)\n",
      "\\xeb\\x8a        |     100.00% | \\xeb\\x8a(100.0%), 게(0.0%), 드를(0.0%)\n",
      "\\xac            |     100.00% | \\xac(100.0%), \\xa4(0.0%), \\x97(0.0%)\n",
      "가               |      29.41% | 처럼(37.8%), 가(29.4%), 를(29.4%)\n",
      " 어              |      96.48% |  어(96.5%),  잘(0.8%),  조(0.7%)\n",
      "우               |      99.75% | 우(99.8%), 울(0.2%),  우(0.0%)\n",
      "러               |     100.00% | 러(100.0%), 려(0.0%), 르는(0.0%)\n",
      "진               |      40.57% | 지는(59.0%), 진(40.6%), 져(0.4%)\n",
      " 느낌             |      90.78% |  느낌(90.8%),  공간(2.7%),  곳(1.5%)\n",
      "을               |      99.23% | 을(99.2%), 이(0.7%), 으로(0.1%)\n",
      " \\xec\\xa4       |      13.46% |  주(36.6%),  줄(32.3%),  \\xec\\xa4(13.5%)\n",
      "\\x8d            |      99.13% | \\x8d(99.1%), \\x98(0.9%), \\x8c(0.0%)\n",
      "니다              |     100.00% | 니다(100.0%), 니(0.0%), 습니다(0.0%)\n",
      ".               |      99.99% | .(100.0%), !(0.0%), .\n",
      "(0.0%)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "prompt = \"새로 오픈한 조용한 북카페 이름을 한글로 딱 하나만 추천해줘.\"\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    logprobs=True,\n",
    "    top_logprobs=3,\n",
    "    max_completion_tokens=50\n",
    ")\n",
    "\n",
    "content = response.choices[0].message.content\n",
    "logprobs_data = response.choices[0].logprobs.content\n",
    "\n",
    "print(f\"질문: {prompt}\")\n",
    "print(f\"답변: {content}\\n\")\n",
    "print(f\"{'Token':<15} | {'Probability':<12} | {'Top Alternatives'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for lp in logprobs_data:\n",
    "    prob = math.exp(lp.logprob) * 100\n",
    "    alternatives = [f\"{top.token}({math.exp(top.logprob)*100:.1f}%)\" for top in lp.top_logprobs]\n",
    "    print(f\"{lp.token:<15} | {prob:>10.2f}% | {', '.join(alternatives)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
